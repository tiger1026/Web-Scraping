{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTT 論壇爬蟲\n",
    "\n",
    "這本jupyter notebook 主要希望做到的幾點\n",
    "1. 成功的爬到八卦版的最新幾頁的文章加上評論\n",
    "2. 抓到評論的：噓，→ ，推\n",
    "3. 做成 pandas Dataframe\n",
    "\n",
    "Main objectives \n",
    "1. scrape post content and comments from PTT\n",
    "2. get the \"attitude\" of comments (possible sentiment analysis)\n",
    "3. df.to_csv() to save as an csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting PTT With Inspector Tools\n",
    "右鍵按inspect來看網頁的HTML code <br>\n",
    "我們會看到我們的第一步其實是要抓出一個List的網站鏈接<br>\n",
    "right click inspect to check code. we need to get a list of urls from div class title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note that this web contains a age restriction page, so we need to navigate pass this page with selenium\n",
    "\n",
    "# set up chrome webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# reqeust\n",
    "url = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "driver.get(url)\n",
    "\n",
    "# click button\n",
    "driver.find_element_by_class_name('btn-big').click()\n",
    "\n",
    "# create beautiful soup\n",
    "pagesource = driver.page_source\n",
    "soup = BeautifulSoup(pagesource)\n",
    "\n",
    "# check our soup\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/bbs/Gossiping/index39092.html'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since the index number of the webpage changes as the number of posts change, lets\n",
    "# find the index of the previous page to set an index number\n",
    "\n",
    "# find all buttons\n",
    "page_navigator = soup.find('div', class_ = 'btn-group btn-group-paging').find_all('a', class_ = 'btn wide')\n",
    "\n",
    "# we only want the previous page button href text\n",
    "url_index = page_navigator[1]['href']\n",
    "url_index  #check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39093"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets get the integer of url_index\n",
    "url_index = int(re.search(r'\\d+', url_index).group())\n",
    "\n",
    "# the url_index + 1 will be our first page\n",
    "url_index = int(url_index) + 1\n",
    "url_index\n",
    "\n",
    "# okay so our page index will be a list that iterates backwards by a decrement of 1 depending on how many pages you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# we see that the class title of each section leads to another page that contains the information we need\n",
    "# right now all we get to see is the titles\n",
    "# we need to find a way to navigate into all the pages\n",
    "# or we can create a tone of independent soups\n",
    "# i will try the latter since im not that great with selenium \n",
    "\n",
    "# find the list of class_ = title\n",
    "post_container = soup.find_all('div', class_ = 'title')\n",
    "print(len(post_container)) # should expect 9 i our case but note it changes due to page size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/bbs/Gossiping/M.1599550604.A.2AF.html'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets try getting all the urls \n",
    "\n",
    "# define a first post\n",
    "first_post = post_container[0]\n",
    "\n",
    "# accest first post href\n",
    "first_post.a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.ptt.cc/bbs/Gossiping/M.1599550604.A.2AF.html'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Great now we just combine this to a base url\n",
    "base_url = 'https://www.ptt.cc'\n",
    "first_post_url = base_url + first_post.a['href']\n",
    "first_post_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Post content\n",
    "After we got the url on the first page we now scrape info of the first post <br>\n",
    "for now we ignore all images in the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now since we have a first post url lets try making a request for this url\n",
    "\n",
    "# set up chrome webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# reqeust\n",
    "driver.get(first_post_url)\n",
    "\n",
    "# click button\n",
    "driver.find_element_by_class_name('btn-big').click()\n",
    "\n",
    "# create beautiful soup\n",
    "pagesource = driver.page_source\n",
    "soup = BeautifulSoup(pagesource)\n",
    "\n",
    "# check our soup\n",
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Post Main Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "<div class=\"article-metaline\"><span class=\"article-meta-tag\">作者</span><span class=\"article-meta-value\">a98987605 (a)</span></div>\n",
      "<div class=\"article-metaline\"><span class=\"article-meta-tag\">標題</span><span class=\"article-meta-value\">[問卦] 故宮的東西都是垃圾吧？</span></div>\n",
      "<div class=\"article-metaline\"><span class=\"article-meta-tag\">時間</span><span class=\"article-meta-value\">Tue Sep  8 15:36:42 2020</span></div>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'標題[問卦] 故宮的東西都是垃圾吧？'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parsing title under <span> class=article-meta-value\n",
    "title_container = soup.find_all('div', class_ = 'article-metaline')\n",
    "\n",
    "# find the one we need which is 1\n",
    "print(len(title_container))\n",
    "for i in range(3):\n",
    "    print(title_container[i])\n",
    "    \n",
    "# title that we want\n",
    "title = title_container[1]\n",
    "title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"bbs-screen bbs-content\" id=\"main-content\"><div class=\"article-metaline\"><span class=\"article-meta-tag\">作者</span><span class=\"article-meta-value\">a98987605 (a)</span></div><div class=\"article-metaline-right\"><span class=\"article-meta-tag\">看板</span><span class=\"article-meta-value\">Gossiping</span></div><div class=\"article-metaline\"><span class=\"article-meta-tag\">標題</span><span class=\"article-meta-value\">[問卦] 故宮的東西都是垃圾吧？</span></div><div class=\"article-metaline\"><span class=\"article-meta-tag\">時間</span><span class=\"article-meta-value\">Tue Sep  8 15:36:42 2020</span></div>\n",
      "那個啊\n",
      "故宮的東西\n",
      "大部分都是從支那過來的，跟垃圾沒兩樣吧？\n",
      "中國 = 垃圾 懂？\n",
      "\n",
      "拿去資源回收或是某些人想要就還他們吧\n",
      "幹嘛要花稅金保存跟展示這些垃圾呀？\n",
      "\n",
      "有沒有卦？\n",
      "\n",
      "--\n",
      "<span class=\"f2\">※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 101.10.109.78 (臺灣)\n",
      "</span><span class=\"f2\">※ 文章網址: <a href=\"https://www.ptt.cc/bbs/Gossiping/M.1599550604.A.2AF.html\" rel=\"nofollow\" target=\"_blank\">https://www.ptt.cc/bbs/Gossiping/M.1599550604.A.2AF.html</a>\n",
      "</span></div>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parsing main content under bbs-scren bbs-content\n",
    "# the problem with the main content is that the text doesnt have a tag\n",
    "# we will need to do a lot of processing to get only the content\n",
    "# However, there is a tag for the section above, hence we can use the next_sibling command to get the text\n",
    "# and instead of finding the entire body, we find the class'richcontent'\n",
    "\n",
    "content = soup.find('div', class_ = 'bbs-screen bbs-content')\n",
    "content = str(content)\n",
    "print(content)\n",
    "type(content)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "content = re.findall(r'[\\u4e00-\\u9fff]+', content)\n",
    "type(content[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# parse comments\n",
    "# comments have identical class called \"push\" hence we use find all\n",
    "comments_container = soup.find_all('div', class_ = 'push')\n",
    "print(len(comments_container)) # should have 11 comments in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a7c8fafefb28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get the first comment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# if there is an error here it means there are no comments so we have to add an if statement for this later when we combine everything\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfirst_comment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomments_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfirst_comment_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_comment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfirst_comment_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_comment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'f3 push-content'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# get the first comment\n",
    "# if there is an error here it means there are no comments (which is very likely as it alwaays scrapes the newest post first)\n",
    "# so we have to add an if statement for this later when we combine everything\n",
    "first_comment = comments_container[0]\n",
    "first_comment_type = first_comment.span.text \n",
    "first_comment_context = first_comment.find('span', class_ = 'f3 push-content').text\n",
    "first_comment_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Everything with Loops\n",
    "\n",
    "Our next step will to try and loop everything together so we can get a larget amount of posts at once <br>\n",
    "for this notebook, we will simply gather all txt information (both post context and comments)<br>\n",
    "this can be used to make word clouds for social listening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request: 184; Frequency: 0.07743006352758633 request/s\n"
     ]
    }
   ],
   "source": [
    "# lets see what we need \n",
    "#     1. the outer laywer will be loop for different pages (indices)\n",
    "#     2. inner should have a loop for \n",
    "#         - different posts url\n",
    "#         - post comments\n",
    "\n",
    "# lets start by declaring our single list variable that will contain all texts of our web scrapping process\n",
    "mylist = []\n",
    "\n",
    "# we want to monitor the loop in to check if we are accessing the page at a human-like rate\n",
    "start_time = time()\n",
    "requests = 0\n",
    "\n",
    "# set up chrome webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# reqeust\n",
    "url = 'https://www.ptt.cc/bbs/Gossiping/index.html'  # can be changed to other ptt sections\n",
    "driver.get(url)\n",
    "\n",
    "# click age restriction button\n",
    "driver.find_element_by_class_name('btn-big').click()\n",
    "\n",
    "# create beautiful soup\n",
    "pagesource = driver.page_source\n",
    "soup = BeautifulSoup(pagesource)\n",
    "\n",
    "# find url index\n",
    "page_navigator = soup.find('div', class_ = 'btn-group btn-group-paging').find_all('a', class_ = 'btn wide')\n",
    "url_index = page_navigator[1]['href']\n",
    "url_index = int(re.search(r'\\d+', url_index).group())\n",
    "url_index = int(url_index) + 1   # the number that should be in our url\n",
    "\n",
    "# Create list\n",
    "index_list = []\n",
    "for i in range(10):       # 100 pages worth of data\n",
    "    index_list.append(url_index)\n",
    "    url_index = url_index - 1\n",
    "\n",
    "# first for loop for index_list\n",
    "for i in index_list:\n",
    "    \n",
    "    # next we want to get a list of urls after we access each page\n",
    "    # first we create a new url from the for loop\n",
    "    url = 'https://www.ptt.cc/bbs/Gossiping/index' + str(i) + '.html'  \n",
    "    driver.get(url)\n",
    "    \n",
    "    # temporarily pause loop with sleep function\n",
    "    sleep(randint(8,15))\n",
    "    \n",
    "    # create beautiful soup\n",
    "    pagesource = driver.page_source\n",
    "    soup = BeautifulSoup(pagesource)\n",
    "    \n",
    "    # Create a post container\n",
    "    post_container = soup.find_all('div', class_ = 'title')\n",
    "    \n",
    "    # create a list for post urls on certain page\n",
    "    posts_url = []\n",
    "    for i in range(len(post_container)):\n",
    "        if post_container[i].find('a') is not None:\n",
    "            posts_url.append(str(post_container[i].a['href']))\n",
    "        \n",
    "    # Now we create a loop that accesses all posts\n",
    "    for i in posts_url:\n",
    "        \n",
    "        # create a new url here\n",
    "        url = 'https://www.ptt.cc' + i\n",
    "        driver.get(url)                            # repeate everything again.....\n",
    "\n",
    "        ###################################### Monitor by pausing and calculating time\n",
    "        \n",
    "        # temporarily pause loop with sleep function\n",
    "        sleep(randint(8,15))\n",
    "\n",
    "        # monitor requests\n",
    "        requests += 1\n",
    "        elapsed_time = time() - start_time\n",
    "        print('Request: {}; Frequency: {} request/s'.format(requests, requests/elapsed_time))\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "        #########################################   Soup\n",
    "        \n",
    "        # create beautiful soup\n",
    "        pagesource = driver.page_source\n",
    "        soup = BeautifulSoup(pagesource)\n",
    "        \n",
    "        ###################################### Main content\n",
    "        \n",
    "        content_all = soup.find('div', class_ = 'bbs-screen bbs-content')    # THIS PART NEEDS IMPROVEMENT\n",
    "        content_all = str(content_all)\n",
    "        content_ch = re.findall(r'[\\u4e00-\\u9fff]+', content_all)  # using regular expression and chinese character unicode we can find all chinese chrarcters only\n",
    "        mylist.append(content_ch)\n",
    "        \n",
    "        ###################################### comments\n",
    "        \n",
    "        comments_container = soup.find_all('div', class_ = 'push')\n",
    "        if len(comments_container) != 0:\n",
    "            for i in range(len(comments_container)):\n",
    "                mylist.append(comments_container[i].find('span', class_ = 'f3 push-content').text)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/bbs/Gossiping/M.1599550186.A.8B5.html\">Re: [新聞]歷史老師看三國史:早期國民黨要同情讚美蜀</a>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets change it to a dataframe\n",
    "df = pd.DataFrame({\n",
    "    'text':mylist,\n",
    "})\n",
    "\n",
    "\n",
    "# save as excel file\n",
    "\n",
    "df.to_excel(r\"C:\\Users\\tiger\\Desktop\\jupyter notebook works\\ptt\\ptt.xlsx.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
